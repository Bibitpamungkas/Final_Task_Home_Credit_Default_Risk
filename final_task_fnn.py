# -*- coding: utf-8 -*-
"""Final Task FNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_4dnrLUzQDZzNSvHOW10ls0qVT5P1PB5

# **Final Task Prediksi Skor Kredit**

## **Pembacaan dan Review Data`**

#### **Melihat Data**

##### **Membaca dan mengambil file yang ada di dalam google drive yang dipakai**

Melakukan penyambungan penyimpanan google drive dan google collab untuk pengambilan data
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd /content/gdrive/My\ Drive/Colab\ Notebooks/rakamin home credit

"""Menampilkan isi yang ada di dalam penyimpanan google drive yang dipakai"""

ls

"""##### **Menampilkan 5 Data Teratas**"""

import pandas as pd
dt = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/rakamin home credit/application_train.csv')
dt.head()

"""##### **Melihat Informasi Data Secara Keseluruhan**

Menampilkan informasi objek
"""

dt.info()

"""Melakukan deteksi validasi nilai pada objek"""

dt.notnull()

"""Melakukan pengecekan jumlah nilai yang hilang pada objek"""

nulls = dt.isnull().sum().to_frame()
for index, row in nulls.iterrows():
  print(index, row[0])

"""mengecek tipe data dari setiap kolom"""

dt.dtypes.value_counts()

"""Mengecek kolom target"""

dt.TARGET.value_counts()

"""## **Data Preprocessing`**

#### **Melakukan Feature Selection**

##### Melakukan penghilangan kolom berdasarkan penilaian missing value

Pengecekan kolom yang memiliki data kosong diatas 50%
"""

import pandas as pd

# Function to calculate missing values by column in a DataFrame
def missing_values_table(df):
    # Calculate total missing values
    mis_val = dt.isnull().sum()

    # Calculate percentage of missing values
    mis_val_percent = 100 * dt.isnull().sum() / len(dt)

    # Make a table with the results
    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)

    # Rename the columns
    mis_val_table_ren_columns = mis_val_table.rename(
    columns = {0 : 'Missing Values', 1 : '% of Total Values'})

    # Sort the table by percentage of missing descending, after excluding columns with no missing values
    mis_val_table_ren_columns = mis_val_table_ren_columns[
        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
    '% of Total Values', ascending=False).round(1)

    # Print some summary information
    print(f"Your selected dataframe has {df.shape[1]} columns.\n"
          f"There are {mis_val_table_ren_columns.shape[0]} columns that have missing values.")

    # Return the dataframe with missing information
    return mis_val_table_ren_columns

# Apply the function to your data
missing_values = missing_values_table(dt)
missing_values.head(20)

"""Pengecekan kolom kategorikal yang memiliki nilai unik"""

unique_counts = dt.select_dtypes('object').apply(pd.Series.nunique)
unique_counts

"""Melakukan penghilangan kolom berdasarkan penilaian data kosong diatas 50%"""

hapus_kolom2 = list(missing_values[missing_values > 0.5].index)
kolom12= set(hapus_kolom2)

dt.drop(kolom12, axis=1, inplace=True)
dt.shape

"""##### Melakukan Pengecekan Missing Value

Melakukan perhitungan total dari data yang kosong
"""

dt.isnull().sum()

"""Menampilkan Heatmap"""

import matplotlib.pyplot as plt
import seaborn as sns

#Check correlation
plt.figure(figsize=(24,24))
sns.heatmap(dt.corr(), annot=True, annot_kws={'size':7})

"""Reset Index"""

dt.reset_index(drop= True, inplace = True)

"""#### **Melakukan Pengecekan Fitur Tipe Data**"""

dt.info()

"""## **Exploratory Data Analysis`**

### **Bar Chart**
"""

def annot_plot(ax,w,h):
  ax.spines['top'].set_visible(False)
  ax.spines['right'].set_visible(False)
  for p in ax.patches:
      ax.annotate('{0:.1f}'.format(p.get_height()), (p.get_x()+w, p.get_height()+h))

import matplotlib.pyplot as plt
import seaborn as sns

ax = sns.countplot(dt ,x='TARGET')
annot_plot(ax, 0.27,1)

plt.title("Loan Value")
plt.xlabel('Repaid and Not Repaid')
plt.ylabel('Total')

plt.show()

pd.crosstab(dt.NAME_INCOME_TYPE,dt['TARGET']).plot(kind="bar",figsize=(15,6),color=['#1CA53B','#AA1111'])
plt.title('Bad and Good Loan Based Grade')
plt.xlabel('Tipe Loan (0 = Bad, 1 = Good)')
plt.xticks(rotation=0)
plt.ylabel('Total')
plt.show()

"""### **Klasifikasi Feature**

Pengelompokkan Feature berdasarkan Numeric dan Categorical
"""

numeric_features = list(dt.select_dtypes(["float64" , "int64",'int32']).columns)
n = numeric_features.index('TARGET')
del n

categorical_features = list(dt.select_dtypes("object").columns)

target = "TARGET"

print(f'numeric_features:\n{numeric_features}\n\ncategorical_features:\n{categorical_features}\n\ntarget:\n{target}')

"""## **Encoding & Normalization`**

### **One Hot Encoding**
"""

from sklearn.preprocessing import OneHotEncoder
cat_cols = [col for col in categorical_features]
onehot_cols = pd.get_dummies(dt[cat_cols], drop_first=True)
onehot_cols

"""### **Normalization**

Normalisasi Data Menggunakan MinMaxScaler
"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

num_cols = [col for col in numeric_features  if col not in cat_cols + ['TARGET']]
normal = pd.DataFrame(scaler.fit_transform(dt[num_cols]), columns=num_cols)

normal

"""### **Final Data**

Data yang sudah selesai dilakukan preprocesing dan EDA
"""

final_data = pd.concat([onehot_cols, normal, dt[['TARGET']]], axis=1)
final_data.head()

"""## **Modeling`**

### **Spliting Data**
"""

from keras.models import Sequential
from keras.layers import Dense,Activation
from keras import layers
import keras
from sklearn.model_selection import train_test_split
import sklearn.metrics as metrics

from sklearn.metrics import accuracy_score,roc_auc_score,precision_score, recall_score,f1_score,classification_report

x = final_data.drop(['TARGET'], axis=1)
y = final_data['TARGET']

X_train, X_test, y_train, y_test = train_test_split(x, y ,test_size=0.2, random_state=7)
y_train.value_counts()

"""### **SMOTE**

Mengatasi Data Immbalance dengan menggunakan salah satu teknik OverSampling
"""

from imblearn.over_sampling import SMOTE
from collections import Counter

smote = SMOTE(random_state=7)
# fit predictor and target variable
x_smote, y_smote = smote.fit_resample(X_train, y_train)


print('Dataset Sebelum di SMOTE', Counter(y_train))
print('Dataset setelah di SMOTE', Counter(y_smote))

"""### **Model Training Data**

Membuat Model Training menggunakan Algoritma Feedforward Neural Networks Classifier
"""

from keras.models import Sequential
from keras.layers import Dense

def create_model():
    # Membuat model Sequential
    model = Sequential()

    # Menambahkan lapisan input dan lapisan tersembunyi pertama
    model.add(Dense(128, activation="relu", input_dim=x_smote.shape[1]))

    # Menambahkan lapisan tersembunyi kedua
    model.add(Dense(64, activation="elu"))

    # Menambahkan lapisan output
    model.add(Dense(units=1, kernel_initializer='uniform', activation="sigmoid"))

    # Mengkompilasi model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Membuat model
classifier = create_model()

# Melatih model
classifier.fit(x_smote, y_smote, batch_size=32, epochs=100)

"""## **Classification Report`**

### **Accuracy**
"""

score, acc = classifier.evaluate(X_test, y_test)
print('Test score: %.2f', (score*100))
print('Test accuracy: %.2f',  (acc*100))

import numpy as np
#ypred = np.argmax(classifier.predict(X_test),axis=1)
ypred_proba = classifier.predict(X_test)
ypred = (ypred_proba > 0.5).astype(int)
print(classification_report(y_test, ypred))

"""### **Confusion Matrix**"""

from sklearn.metrics import confusion_matrix
c_matrix = confusion_matrix(y_test, ypred)
fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))

sns.heatmap(pd.DataFrame(c_matrix), annot=True, cmap="Blues" ,fmt='g',
            xticklabels=['Paid', 'Not Paid'],
            yticklabels=['Paid', 'Not Paid'],)
ax1.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion Matrix', y=1.1,fontsize=14)
plt.show()

"""## **HyperParameter Tunning**

### **GridSearch**
"""

!pip install scikeras
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import GridSearchCV


classifier = KerasClassifier(build_fn=create_model, verbose=0)
# defining parameter range
param_grid = {'batch_size': [64],
              'epochs': [100],
              'optimizer': ['Nadam']
}

grid = GridSearchCV(estimator=classifier, param_grid=param_grid, n_jobs=-1, scoring='accuracy')

# fitting the model for grid search
grid.fit(X_train, y_train)

# print best parameter after tuning
print(grid.best_params_)
print("Best score: ", (grid.best_score_*100))

# print how our model looks after hyper-parameter tuning
print(grid.best_estimator_)