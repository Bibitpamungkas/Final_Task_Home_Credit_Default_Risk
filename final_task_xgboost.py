# -*- coding: utf-8 -*-
"""Final Task XGBoost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tzvbnU0hvp8WjrdA_1QAfsQbYFqBtBsu

# **Final Task Prediksi Skor Kredit**

## **Pembacaan dan Review Data`**

#### **Melihat Data**

##### **Membaca dan mengambil file yang ada di dalam google drive yang dipakai**

Melakukan penyambungan penyimpanan google drive dan google collab untuk pengambilan data
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd /content/gdrive/My\ Drive/Colab\ Notebooks/rakamin home credit

"""Menampilkan isi yang ada di dalam penyimpanan google drive yang dipakai"""

ls

"""##### **Menampilkan 5 Data Teratas**"""

import pandas as pd
dt = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/rakamin home credit/application_train.csv')
dt.head()

"""##### **Melihat Informasi Data Secara Keseluruhan**

Menampilkan informasi objek
"""

dt.info()

"""Melakukan deteksi validasi nilai pada objek"""

dt.notnull()

"""Melakukan pengecekan jumlah nilai yang hilang pada objek"""

nulls = dt.isnull().sum().to_frame()
for index, row in nulls.iterrows():
  print(index, row[0])

"""mengecek tipe data dari setiap kolom"""

dt.dtypes.value_counts()

"""Mengecek kolom target"""

dt.TARGET.value_counts()

"""## **Data Preprocessing`**

#### **Melakukan Feature Selection**

##### Melakukan penghilangan kolom berdasarkan penilaian missing value

Pengecekan kolom yang memiliki data kosong
"""

import pandas as pd

# Function to calculate missing values by column in a DataFrame
def missing_values_table(df):
    # Calculate total missing values
    mis_val = dt.isnull().sum()

    # Calculate percentage of missing values
    mis_val_percent = 100 * dt.isnull().sum() / len(dt)

    # Make a table with the results
    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)

    # Rename the columns
    mis_val_table_ren_columns = mis_val_table.rename(
    columns = {0 : 'Missing Values', 1 : '% of Total Values'})

    # Sort the table by percentage of missing descending, after excluding columns with no missing values
    mis_val_table_ren_columns = mis_val_table_ren_columns[
        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
    '% of Total Values', ascending=False).round(1)

    # Print some summary information
    print(f"Your selected dataframe has {df.shape[1]} columns.\n"
          f"There are {mis_val_table_ren_columns.shape[0]} columns that have missing values.")

    # Return the dataframe with missing information
    return mis_val_table_ren_columns

# Apply the function to your data
missing_values = missing_values_table(dt)
missing_values.head(20)

"""Pengecekan kolom kategorikal yang memiliki nilai unik"""

unique_counts = dt.select_dtypes('object').apply(pd.Series.nunique)
unique_counts

"""Melakukan penghilangan kolom berdasarkan penilaian data kosong diatas 50%"""

hapus_kolom2 = list(missing_values[missing_values > 0.5].index)
kolom12= set(hapus_kolom2)

dt.drop(kolom12, axis=1, inplace=True)
dt.shape

"""##### Melakukan Pengecekan Missing Value

Melakukan perhitungan total dari data yang kosong
"""

dt.isnull().sum()

"""Menampilkan Heatmap"""

import matplotlib.pyplot as plt
import seaborn as sns

#Check correlation
plt.figure(figsize=(24,24))
sns.heatmap(dt.corr(), annot=True, annot_kws={'size':7})

"""Reset Index"""

dt.reset_index(drop= True, inplace = True)

"""#### **Melakukan Pengecekan Fitur Tipe Data**"""

dt.info()

"""## **Exploratory Data Analysis`**

### **Bar Chart**
"""

def annot_plot(ax,w,h):
  ax.spines['top'].set_visible(False)
  ax.spines['right'].set_visible(False)
  for p in ax.patches:
      ax.annotate('{0:.1f}'.format(p.get_height()), (p.get_x()+w, p.get_height()+h))

import matplotlib.pyplot as plt
import seaborn as sns

ax = sns.countplot(dt ,x='TARGET')
annot_plot(ax, 0.27,1)

plt.title("Loan Value")
plt.xlabel('Repaid and Not Repaid')
plt.ylabel('Total')

plt.show()

pd.crosstab(dt.NAME_INCOME_TYPE,dt['TARGET']).plot(kind="bar",figsize=(15,6),color=['#1CA53B','#AA1111'])
plt.title('Tipe Loan Based Income Type')
plt.xlabel('Tipe Loan (0 = Not Repaid, 1 = Repaid)')
plt.xticks(rotation=0)
plt.ylabel('Total')
plt.show()

import plotly.express as px

fig = px.pie(dt, values='TARGET', names='NAME_EDUCATION_TYPE')
fig.update_layout(title_text='Tipe Loan Based Education Type')
fig.show()

"""### **Klasifikasi Feature**

Pengelompokkan Feature berdasarkan Numeric dan Categorical
"""

numeric_features = list(dt.select_dtypes(["float64" , "int64",'int32']).columns)
n = numeric_features.index('TARGET')
del n

categorical_features = list(dt.select_dtypes("object").columns)

target = "TARGET"

print(f'numeric_features:\n{numeric_features}\n\ncategorical_features:\n{categorical_features}\n\ntarget:\n{target}')

"""## **Encoding & Normalization`**

### **One Hot Encoding**
"""

from sklearn.preprocessing import OneHotEncoder
cat_cols = [col for col in categorical_features]
onehot_cols = pd.get_dummies(dt[cat_cols], drop_first=True)
onehot_cols

"""### **Normalization**

Normalisasi Data Menggunakan MinMaxScaler
"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

num_cols = [col for col in numeric_features  if col not in cat_cols + ['TARGET']]
normal = pd.DataFrame(scaler.fit_transform(dt[num_cols]), columns=num_cols)

normal

"""### **Final Data**

Data yang sudah selesai dilakukan preprocesing dan EDA
"""

final_data = pd.concat([onehot_cols, normal, dt[['TARGET']]], axis=1)
final_data.head()

"""## **Modeling`**

### **Spliting Data**
"""

from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
import sklearn.metrics as metrics

from sklearn.metrics import accuracy_score,roc_auc_score,precision_score, recall_score,f1_score,classification_report

x = final_data.drop(['TARGET'], axis=1)
y = final_data['TARGET']

X_train, X_test, y_train, y_test = train_test_split(x, y ,test_size=0.2, random_state=7)
y_train.value_counts()

"""### **SMOTE**

Mengatasi Data Immbalance dengan menggunakan salah satu teknik OverSampling
"""

from imblearn.over_sampling import SMOTE
from collections import Counter

smote = SMOTE(random_state=7)
# fit predictor and target variable
x_smote, y_smote = smote.fit_resample(X_train, y_train)


print('Dataset Sebelum di SMOTE', Counter(y_train))
print('Dataset setelah di SMOTE', Counter(y_smote))

"""### **Model Training Data**

Membuat Model Training menggunakan Algoritma XGBoost Classifier
"""

model = XGBClassifier(
    learning_rate =0.1,
    n_estimators=1000,
    subsample=0.8,
    colsample_bytree=0.8,
    objective= 'binary:logistic'
    )
model.fit(x_smote, y_smote)

"""## **Classification Report`**

### **Accuracy**
"""

y_pred = model.predict(X_test)
predictions = [round(value) for value in y_pred]

#accuracy
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

#precision
precision = precision_score(y_test, predictions)
print("Precision : %.2f%%" % (precision * 100.0))

#recall
recall = recall_score(y_test, y_pred)
print("Recall : ",recall )

#F1 SCORE
f1score = f1_score(y_test.T,y_pred, average='macro')
print("F1 Score : ",f1score )

"""### **Confusion Matrix**"""

c_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])
fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))

sns.heatmap(pd.DataFrame(c_matrix), annot=True, cmap="Blues" ,fmt='g',
            xticklabels=['Paid', 'Not Paid'],
            yticklabels=['Paid', 'Not Paid'],)
ax1.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion Matrix', y=1.1,fontsize=14)
plt.show()

from sklearn.metrics import classification_report
ypred = model.predict(X_test)
print(classification_report(y_test, ypred))

"""## **HyperParameter Tunning**

### **GridSearch**
"""

from sklearn.model_selection import GridSearchCV

# defining parameter range
param_grid = {
    'max_depth': [5, 7],
    'subsample': [0.6, 0.8],
    'gamma':[0],
    'min_child_weight':[1]
}

grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')

# fitting the model for grid search
grid.fit(x_smote, y_smote)

# print best parameter after tuning
print(grid.best_params_)
print("Best score: ", grid.best_score_)

# print how our model looks after hyper-parameter tuning
print(grid.best_estimator_)